---
layout: post
title: When is TSLS Actually LATE?
---

Good friends don't let good friends use IV. Many argue that this one-liner hasn't been more true than today. Instrumental variable approaches and two-stage least square estimators (TSLS) have been at the forefront of
the credibility revolution in economics. It basically allows you to exploit variation, that is unrelated to the outcome of interest but nevertheless causes you to take up the treatment. A famous example
are military lotteries. If you are interested in the effect of you military service on your earnings, you are probably confronted with the fact that those in the military differ from the general working population, and 
that these factors potentially correlate with their earnings. Hence, a simple difference between those who served in the military and those who ddi not gives you a biased estimate. To circumvent this issue,
researchers exploited random draw policies (among those who are eligible) to check the causal effect  of a military service on earnings. By the virtue of randomization, the treatment and control group are identical (in a distributional sense) in their
observed and unobserved characteristics. Hence, we can recover an unbiased estimate of the causal estimate of serving the military.

Typically, individuals have heterogenours treatment effects. As a consequence, the compliance to the instrument, i.e., ``winning'' the lottery, will be imperfect. Under this assumption, one has to invoke the assumption of monotonicity.
This means, the instrument has to change particpiation in the treatment in one direction only. To put it differently, in a binary treatment case with binary instrument, we have to be able to rule out defiers. This is a group of
individuals who would take up the treatment if not assigned to it, but deny treatment if assigned to it. This assumption is usually not testable, but can be rationalized by insitutional knowledge. Under this assumption,
we can recover a parameter of interest, the local average treatment effect (LATE). This is the treatment effect among those who take up the treatment if incentivized by the instrument, but would not otherwise. A lot of the
literature is centered on estimating this parameter.

In practice, many researchers control for a wide range of covariates, either to increase the efficiency of the estimates or to make the exogeneity assumption more credible. However, this practice, or at least the way it was implemented, is questioned by Blandhol et al. (2022) now. The
reason is that the IV estimand can be decomposed into a weighted average of the effect of the compliers and the always/never takers. The weights on the latter depends on the expectation of the instrument, conditional on the set of covariates, and the linear
projection of the instrument on the set of covariates. This difference can be either zero, positive or negative. And since the sign of the weights on the always/never-takes depends on the sign of this difference, the weights on these groups' treatment effects
would be negative if the difference is negative. This results in a distortion of the estimand of interest and in the worst case, to a change of the sign of the treatment effect of interest. And this is obviously undesirable. The very least we expect from a good estimator is 
that it is ``weakly causal''. This is a notation introduced by Blandhol et al. (2022). It means, if among all groups, e.g., compliers, never takers and so forth, each subsequent treatment contrast (effect) is positive (negative) or zero,
we would like that our estimand is also positive (negative) or zero. The only way to ensure that the weights are non-negative is to saturate the set of covariates, to include rich covariates. The authors show that this is a sufficient and necessary condition to have
a weakly causal estimand in the case of homogenous treatment effects. Under the assumption of heterogneous effects, one has to add ``monotinicity correction'' to these requirements. That is, we have to fully interact our covariates with the
instrument. This ensures that monotonicity holds.

## And now?

The implication of this for empirical is to include rich covariates into IV models. An alternative approach is to correctly specificy the functional form between the (excluded) instrument and the covariates and justify it. And do not forget to monotonicity
correct your estimates. I think completely saturating your model is the way to go. However, I think this increases requirements to data quality since some cells of your covariate combinations might be sparsely filled.


## References

 Blandhol, Christine and Bonney, John and Mogstad, Magne and Torgovitsky, Alexander, When is TSLS Actually LATE? (February 9, 2022). University of Chicago, Becker Friedman Institute for Economics Working Paper No. 2022-16, Available at SSRN: https://ssrn.com/abstract=4014707 or http://dx.doi.org/10.2139/ssrn.4014707 

  